System Name: mild-duck
Description (short description of the system): We used a linear combination of a PL2 scoring and a BM25 scoring with Bo1 query expansion. 
The BM25’s score is weighted twice as much as the PL2’s score. Next, the top50 results are reranked with the mono-t5 transformer using the monot5-base-msmarco1 model. 
After the reranking process, we aggregated the snippets with max passage.

************
System Details. Please provide as many details as possible here, so it is possible to reproduce your run.

Ranking Methods (Which ranking approaches does the system use?): PL2, BM25 with Bo1 query expansion and reranking with MonoT5 with max passage aggregation
Data Used (Which data were used to train and fine-tune the system? Please be as concrete as possible and use the exact reference whenever possible): TODO
Software Used (Which software and tools did you use for training, tunning and running your system? Please be as concrete as possible, provide a reference to your code if possible, and provide the exact software version, whenever applicable): python 3.10 with pyterrier 0.1.0, pyterrier_t5, tira 0.0.88
Pre-processing and Indexing (What pre-processing and indexing does the system use? Please be as concrete as possible and provide the details and the setup of the tools): Pyterrier Indexer with default settings
System Combination / Fusion (Does the system combine different retrieval models? If so, how are they combined?): We used a linear combination of a PL2 scoring and a BM25 scoring with Bo1 query expansion. 
Multi-stage Retrieval (Is system single-stage or does it use reranking? If multi-stage, which rerankers are used?): multi-stage with MonoT5 with max passage aggregation
Translations (Does the system use French documents or the translations? If translations, which ones?): no French documents were used

Resources (How much GPU, CPU, memory, ... did you use for pre-processing and inference steps? Did you use any commercial cloud services?): TODO
The Costs (How long did pre-processing and inference take?): TODO

************
Yes/No Questions

Did you use any statistical ranking model? (yes/no): yes (TODO)
Did you use any deep neural network model? (yes/no): yes (TODO)
Did you use a sparse neural model? (yes/no): no (TODO)
Did you use a dense neural model? (yes/no): yes (TODO)
Did you use more than a single retrieval model? (yes/no): yes
Did you use French or English documents (French/English/both): English
Did you use provided English translations (yes/no): no
Did you use any manual intervention on the translations? (yes/no): no
Did you use any manual intervention on the results? (yes/no): no
