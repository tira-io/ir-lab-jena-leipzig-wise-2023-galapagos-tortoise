{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tira.third_party_integrations import ensure_pyterrier_is_loaded, normalize_run, persist_and_normalize_run\n",
    "from tira.rest_api_client import Client\n",
    "import pyterrier as pt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from statistics import mean\n",
    "from ranx import fuse, Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensure_pyterrier_is_loaded()\n",
    "tira = Client()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "LONGEVAL_LAGS = {\n",
    "    'lag1': {\n",
    "        'tira_id': 'longeval-2023-01-20240423-training',\n",
    "        'pt_dataset': pt.get_dataset('irds:ir-lab-padua-2024/longeval-2023-01-20240426-training'),\n",
    "    },\n",
    "    'lag6': {\n",
    "        'tira_id': 'longeval-2023-06-20240418-training',\n",
    "        'pt_dataset': pt.get_dataset('irds:ir-lab-padua-2024/longeval-2023-06-20240422-training'),\n",
    "    },\n",
    "    'lag8': {\n",
    "        'tira_id': 'longeval-2023-08-20240418-training',\n",
    "        'pt_dataset': pt.get_dataset('irds:ir-lab-padua-2024/longeval-2023-08-20240422-training'),\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_ranx_dict(run):\n",
    "    ret = {}\n",
    "    for _, row in run.iterrows():\n",
    "        if row['qid'] not in ret:\n",
    "            ret[row['qid']] = {}\n",
    "\n",
    "        ret[row['qid']][row['docno']] = row['score']\n",
    "\n",
    "    return Run(ret)\n",
    "\n",
    "def from_ranx(run):\n",
    "    ret = []\n",
    "\n",
    "    for qid in run.keys():\n",
    "        for docno in run[qid].keys():\n",
    "            ret += [{\n",
    "                'qid': qid,\n",
    "                'docno': docno,\n",
    "                'score': run[qid][docno]\n",
    "            }]\n",
    "    return pd.DataFrame(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [01:52<00:00, 37.61s/it]\n"
     ]
    }
   ],
   "source": [
    "def get_df_with_sparse_cross_encoder_passage_scores(dataset_id, aggregation, system_name):\n",
    "    predictions = tira.pd.from_retriever_submission('ir-benchmarks/fschlatt/abstract-metaball', dataset_id)\n",
    "    ret = {}\n",
    "\n",
    "    for _, row in predictions.iterrows():\n",
    "        qid = row['qid'].split('___split')[0]\n",
    "        docno = row['docno'].split('___p')[0]\n",
    "        score = row['score']\n",
    "        if qid not in ret:\n",
    "            ret[qid] = {}\n",
    "        if docno not in ret[qid]:\n",
    "            ret[qid][docno] = []\n",
    "        \n",
    "        ret[qid][docno] += [score]\n",
    "    \n",
    "    ret_df = []\n",
    "    for qid in ret.keys():\n",
    "        for docno in ret[qid].keys():\n",
    "            score = aggregation(ret[qid][docno])\n",
    "            ret_df += [{\n",
    "                'qid': qid,\n",
    "                'docno': docno,\n",
    "                'score': score,\n",
    "                'system': system_name\n",
    "            }]\n",
    "    return normalize_run(pd.DataFrame(ret_df), system_name)\n",
    "\n",
    "for lag in tqdm(LONGEVAL_LAGS.keys()):\n",
    "    system = 'max-cross-encoder-passage-score'\n",
    "    df_1 = get_df_with_sparse_cross_encoder_passage_scores(LONGEVAL_LAGS[lag]['tira_id'], max, system)\n",
    "    #df_1 = tira.pd.from_retriever_submission('ir-benchmarks/tira-ir-starter/ColBERT Re-Rank (tira-ir-starter-pyterrier)', LONGEVAL_LAGS[lag]['tira_id'])\n",
    "    df_2 = tira.pd.from_retriever_submission('workshop-on-open-web-search/fschlatt/rank-zephyr', LONGEVAL_LAGS[lag]['tira_id'])\n",
    "    df_2 = normalize_run(df_2, system)\n",
    "\n",
    "    #df_3 = tira.pd.from_retriever_submission('ir-benchmarks/tira-ir-starter/MonoT5 Base (tira-ir-starter-gygaggle)', LONGEVAL_LAGS[lag]['tira_id'])\n",
    "    \n",
    "\n",
    "    df = fuse(runs=[to_ranx_dict(df_1), to_ranx_dict(df_2)], method=\"rrf\")\n",
    "    df = from_ranx(df)\n",
    "    normalize_run(df, system).to_csv(f'{system}.{lag}', sep=\" \", header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sec(a):\n",
    "    return sorted(a, reverse=True)[:2][-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
