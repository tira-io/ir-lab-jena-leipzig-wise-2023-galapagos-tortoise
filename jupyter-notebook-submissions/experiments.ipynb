{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTerrier Notebook for Full-Rank Submissions\n",
    "\n",
    "This notebook serves as a baseline full-rank submission for [TIRA](https://tira.io)/[TIREx](https://tira.io/tirex) that builds a PyTerrier index and subsequently creates a run with BM25."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Ensure Libraries are Imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-terrier in /usr/local/python/3.10.13/lib/python3.10/site-packages (0.10.0)\n",
      "Requirement already satisfied: tira==0.0.88 in /usr/local/python/3.10.13/lib/python3.10/site-packages (0.0.88)\n",
      "Requirement already satisfied: ir_datasets in /usr/local/python/3.10.13/lib/python3.10/site-packages (0.5.5)\n",
      "Requirement already satisfied: requests==2.*,>=2.26 in /home/codespace/.local/lib/python3.10/site-packages (from tira==0.0.88) (2.31.0)\n",
      "Requirement already satisfied: docker==6.*,>=6.0.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tira==0.0.88) (6.1.3)\n",
      "Requirement already satisfied: pandas in /home/codespace/.local/lib/python3.10/site-packages (from tira==0.0.88) (2.1.3)\n",
      "Requirement already satisfied: packaging>=14.0 in /home/codespace/.local/lib/python3.10/site-packages (from docker==6.*,>=6.0.0->tira==0.0.88) (23.2)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from docker==6.*,>=6.0.0->tira==0.0.88) (2.0.7)\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in /home/codespace/.local/lib/python3.10/site-packages (from docker==6.*,>=6.0.0->tira==0.0.88) (1.6.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.10/site-packages (from requests==2.*,>=2.26->tira==0.0.88) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.10/site-packages (from requests==2.*,>=2.26->tira==0.0.88) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.10/site-packages (from requests==2.*,>=2.26->tira==0.0.88) (2023.7.22)\n",
      "Requirement already satisfied: numpy in /home/codespace/.local/lib/python3.10/site-packages (from python-terrier) (1.26.2)\n",
      "Requirement already satisfied: wget in /usr/local/python/3.10.13/lib/python3.10/site-packages (from python-terrier) (3.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/python/3.10.13/lib/python3.10/site-packages (from python-terrier) (4.66.1)\n",
      "Requirement already satisfied: pyjnius>=1.4.2 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from python-terrier) (1.6.1)\n",
      "Requirement already satisfied: matchpy in /usr/local/python/3.10.13/lib/python3.10/site-packages (from python-terrier) (0.5.5)\n",
      "Requirement already satisfied: scikit-learn in /home/codespace/.local/lib/python3.10/site-packages (from python-terrier) (1.3.2)\n",
      "Requirement already satisfied: deprecated in /usr/local/python/3.10.13/lib/python3.10/site-packages (from python-terrier) (1.2.14)\n",
      "Requirement already satisfied: chest in /usr/local/python/3.10.13/lib/python3.10/site-packages (from python-terrier) (0.2.3)\n",
      "Requirement already satisfied: scipy in /home/codespace/.local/lib/python3.10/site-packages (from python-terrier) (1.11.3)\n",
      "Requirement already satisfied: joblib in /home/codespace/.local/lib/python3.10/site-packages (from python-terrier) (1.3.2)\n",
      "Requirement already satisfied: nptyping==1.4.4 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from python-terrier) (1.4.4)\n",
      "Requirement already satisfied: more-itertools in /usr/local/python/3.10.13/lib/python3.10/site-packages (from python-terrier) (10.1.0)\n",
      "Requirement already satisfied: jinja2 in /home/codespace/.local/lib/python3.10/site-packages (from python-terrier) (3.1.2)\n",
      "Requirement already satisfied: statsmodels in /usr/local/python/3.10.13/lib/python3.10/site-packages (from python-terrier) (0.14.1)\n",
      "Requirement already satisfied: ir-measures>=0.3.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from python-terrier) (0.3.3)\n",
      "Requirement already satisfied: dill in /usr/local/python/3.10.13/lib/python3.10/site-packages (from python-terrier) (0.3.7)\n",
      "Requirement already satisfied: pytrec-eval-terrier>=0.5.3 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from python-terrier) (0.5.6)\n",
      "Requirement already satisfied: typish>=1.7.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from nptyping==1.4.4->python-terrier) (1.9.3)\n",
      "Requirement already satisfied: beautifulsoup4>=4.4.1 in /home/codespace/.local/lib/python3.10/site-packages (from ir_datasets) (4.12.2)\n",
      "Requirement already satisfied: inscriptis>=2.2.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from ir_datasets) (2.3.2)\n",
      "Requirement already satisfied: lxml>=4.5.2 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from ir_datasets) (4.9.3)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /home/codespace/.local/lib/python3.10/site-packages (from ir_datasets) (6.0.1)\n",
      "Requirement already satisfied: trec-car-tools>=2.5.4 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from ir_datasets) (2.6)\n",
      "Requirement already satisfied: lz4>=3.1.10 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from ir_datasets) (4.3.2)\n",
      "Requirement already satisfied: warc3-wet>=0.2.3 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from ir_datasets) (0.2.3)\n",
      "Requirement already satisfied: warc3-wet-clueweb09>=0.2.5 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from ir_datasets) (0.2.5)\n",
      "Requirement already satisfied: zlib-state>=0.1.3 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from ir_datasets) (0.1.6)\n",
      "Requirement already satisfied: ijson>=3.1.3 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from ir_datasets) (3.2.3)\n",
      "Requirement already satisfied: pyautocorpus>=0.1.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from ir_datasets) (0.1.12)\n",
      "Requirement already satisfied: unlzw3>=0.2.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from ir_datasets) (0.2.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/codespace/.local/lib/python3.10/site-packages (from beautifulsoup4>=4.4.1->ir_datasets) (2.5)\n",
      "Requirement already satisfied: cwl-eval>=1.0.10 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from ir-measures>=0.3.1->python-terrier) (1.0.12)\n",
      "Requirement already satisfied: cbor>=1.0.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from trec-car-tools>=2.5.4->ir_datasets) (1.0.0)\n",
      "Requirement already satisfied: heapdict in /usr/local/python/3.10.13/lib/python3.10/site-packages (from chest->python-terrier) (1.0.1)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from deprecated->python-terrier) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/codespace/.local/lib/python3.10/site-packages (from jinja2->python-terrier) (2.1.3)\n",
      "Requirement already satisfied: multiset<3.0,>=2.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from matchpy->python-terrier) (2.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/codespace/.local/lib/python3.10/site-packages (from pandas->tira==0.0.88) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/codespace/.local/lib/python3.10/site-packages (from pandas->tira==0.0.88) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/codespace/.local/lib/python3.10/site-packages (from pandas->tira==0.0.88) (2023.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/codespace/.local/lib/python3.10/site-packages (from scikit-learn->python-terrier) (3.2.0)\n",
      "Requirement already satisfied: patsy>=0.5.4 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from statsmodels->python-terrier) (0.5.4)\n",
      "Requirement already satisfied: six in /home/codespace/.local/lib/python3.10/site-packages (from patsy>=0.5.4->statsmodels->python-terrier) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Detect if we are in the TIRA sandbox\n",
    "# Install the required dependencies if we are not in the sandbox.\n",
    "if 'TIRA_DATASET_ID' not in os.environ:\n",
    "    !pip3 install  python-terrier tira==0.0.88 ir_datasets\n",
    "    !pip3 install -q python-terrier\n",
    "    !pip3 install -q --upgrade git+https://github.com/terrierteam/pyterrier_t5.git\n",
    "else:\n",
    "    print('We are in the TIRA sandbox.')\n",
    "\n",
    "from tira.third_party_integrations import ensure_pyterrier_is_loaded, persist_and_normalize_run\n",
    "\n",
    "# PyTerrier must be imported after the call to ensure_pyterrier_is_loaded in TIRA.\n",
    "import pyterrier as pt\n",
    "\n",
    "#import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure Pyterrier integration is loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Due to execution in TIRA, I have patched ir_datasets to always return the single input dataset mounted to the sandbox.\n"
     ]
    }
   ],
   "source": [
    "ensure_pyterrier_is_loaded()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load data, create index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No settings given in /home/codespace/.tira/.tira-settings.json. I will use defaults.\n",
      "No settings given in /home/codespace/.tira/.tira-settings.json. I will use defaults.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ir-lab-jena-leipzig-wise-2023/validation-20231104-training documents:   0%|          | 19/61307 [00:00<05:27, 186.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No settings given in /home/codespace/.tira/.tira-settings.json. I will use defaults.\n",
      "No settings given in /home/codespace/.tira/.tira-settings.json. I will use defaults.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ir-lab-jena-leipzig-wise-2023/validation-20231104-training documents: 100%|██████████| 61307/61307 [00:46<00:00, 1312.13it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = pt.get_dataset('irds:ir-lab-jena-leipzig-wise-2023/validation-20231104-training')\n",
    "qrels = dataset.get_qrels()\n",
    "topics = dataset.get_topics(variant=\"title\")\n",
    "topics = topics.head(5)\n",
    "\n",
    "index_loc = \"./index4\"\n",
    "indexer = pt.IterDictIndexer(index_loc)\n",
    "indexref = indexer.index(dataset.get_corpus_iter())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For retrieval with PL2 c=1.2 seems to work best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl2 = pt.BatchRetrieve(indexer, wmodel=\"PL2\", verbose=True, controls={\"b\" : 1.2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, we want to experiment with query expansion for retrieval with BM25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25 = pt.BatchRetrieve(indexer, wmodel=\"BM25\", verbose=True, controls={\"b\" : 0.8})\n",
    "#bm25= ~bm25\n",
    "\n",
    "bo1_expansion = ~bm25 >> pt.rewrite.Bo1QueryExpansion(indexer)\n",
    "bm25_bo1 = bo1_expansion >> bm25\n",
    "\n",
    "kl = ~bm25 >> pt.rewrite.KLQueryExpansion(indexer)\n",
    "bm25_kl = kl >> bm25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additionally, we linearly combine retrieval with BM25 and PL2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BR(BM25): 100%|██████████| 5/5 [00:00<00:00, 25.01q/s]\n",
      "BR(BM25): 100%|██████████| 5/5 [00:00<00:00, 20.47q/s]\n",
      "BR(PL2): 100%|██████████| 5/5 [00:00<00:00, 20.90q/s]\n"
     ]
    }
   ],
   "source": [
    "bm25_bo1_pl2 = (2* bm25_bo1 + pl2)\n",
    "bm25_kl_pl2 = (2* bm25_kl + pl2).transform(topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's run an experiment, to see which retrieval model works best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The combination of BM25 with Bo1 query expansion and PL2 performs slightly better than the others. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, we want to rerank the output with a transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5.py:240: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from pyterrier_t5 import MonoT5ReRanker\n",
    "monoT5 = MonoT5ReRanker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No settings given in /home/codespace/.tira/.tira-settings.json. I will use defaults.\n",
      "No settings given in /home/codespace/.tira/.tira-settings.json. I will use defaults.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ir-lab-jena-leipzig-wise-2023/validation-20231104-training documents:   3%|▎         | 1993/61307 [00:00<00:02, 19927.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No settings given in /home/codespace/.tira/.tira-settings.json. I will use defaults.\n",
      "No settings given in /home/codespace/.tira/.tira-settings.json. I will use defaults.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ir-lab-jena-leipzig-wise-2023/validation-20231104-training documents: 100%|██████████| 61307/61307 [00:03<00:00, 18118.30it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "corpus = pd.DataFrame(dataset.get_corpus_iter())\n",
    "\n",
    "\n",
    "class GetText(pt.Transformer):\n",
    "    def transform(self, topics_or_res: pd.DataFrame) -> pd.DataFrame:\n",
    "        return pd.merge(topics_or_res, corpus, on=\"docno\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyterrier_t5 import T5Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rerank with transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bm25_bo1_pl2_monot5 = (bm25_bo1_pl2 % 10 >> GetText()\n",
    "        >> pt.text.sliding(length=400, stride=64, prepend_attr=None, text_attr=\"text\")\n",
    "        >> monoT5\n",
    "        >> pt.text.max_passage())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create run\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BR(BM25): 100%|██████████| 5/5 [00:00<00:00, 53.98q/s]\n",
      "BR(BM25): 100%|██████████| 5/5 [00:00<00:00,  9.88q/s]\n",
      "BR(PL2): 100%|██████████| 5/5 [00:00<00:00, 62.89q/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calling sliding on df of 50 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "monoT5:   0%|          | 0/103 [00:00<?, ?batches/s]Token indices sequence length is longer than the specified maximum sequence length for this model (549 > 512). Running this sequence through the model will result in indexing errors\n",
      "monoT5:   0%|          | 0/103 [00:09<?, ?batches/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "unknown parameter type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/pyterrier_t5/__init__.py:61\u001b[0m, in \u001b[0;36mMonoT5ReRanker.transform\u001b[0;34m(self, run)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 61\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43menc\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m     62\u001b[0m result \u001b[38;5;241m=\u001b[39m result[:, \u001b[38;5;241m0\u001b[39m, (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mREL, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mNREL)]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:1706\u001b[0m, in \u001b[0;36mT5ForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1704\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m encoder_outputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1705\u001b[0m     \u001b[38;5;66;03m# Convert encoder inputs in embeddings if needed\u001b[39;00m\n\u001b[0;32m-> 1706\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1707\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1708\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1709\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1710\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1711\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1712\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1713\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1714\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(encoder_outputs, BaseModelOutput):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:1110\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1109\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1110\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1112\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1113\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1114\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1120\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1121\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1122\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1124\u001b[0m \u001b[38;5;66;03m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m \u001b[38;5;66;03m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:694\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m    692\u001b[0m     self_attn_past_key_value, cross_attn_past_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 694\u001b[0m self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    703\u001b[0m hidden_states, present_key_value_state \u001b[38;5;241m=\u001b[39m self_attention_outputs[:\u001b[38;5;241m2\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:601\u001b[0m, in \u001b[0;36mT5LayerSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    600\u001b[0m normed_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(hidden_states)\n\u001b[0;32m--> 601\u001b[0m attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSelfAttention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnormed_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    610\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attention_output[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:526\u001b[0m, in \u001b[0;36mT5Attention.forward\u001b[0;34m(self, hidden_states, mask, key_value_states, position_bias, past_key_value, layer_head_mask, query_length, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    523\u001b[0m key_states \u001b[38;5;241m=\u001b[39m project(\n\u001b[1;32m    524\u001b[0m     hidden_states, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk, key_value_states, past_key_value[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    525\u001b[0m )\n\u001b[0;32m--> 526\u001b[0m value_states \u001b[38;5;241m=\u001b[39m \u001b[43mproject\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_value_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m    528\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[38;5;66;03m# compute scores\u001b[39;00m\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:497\u001b[0m, in \u001b[0;36mT5Attention.forward.<locals>.project\u001b[0;34m(hidden_states, proj_layer, key_value_states, past_key_value)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key_value_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    495\u001b[0m     \u001b[38;5;66;03m# self-attn\u001b[39;00m\n\u001b[1;32m    496\u001b[0m     \u001b[38;5;66;03m# (batch_size, n_heads, seq_length, dim_per_head)\u001b[39;00m\n\u001b[0;32m--> 497\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproj_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    499\u001b[0m     \u001b[38;5;66;03m# cross-attn\u001b[39;00m\n\u001b[1;32m    500\u001b[0m     \u001b[38;5;66;03m# (batch_size, n_heads, seq_length, dim_per_head)\u001b[39;00m\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:486\u001b[0m, in \u001b[0;36mT5Attention.forward.<locals>.shape\u001b[0;34m(states)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"projection\"\"\"\u001b[39;00m\n\u001b[0;32m--> 486\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mstates\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_value_proj_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: unknown parameter type",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCreate run\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m run \u001b[38;5;241m=\u001b[39m \u001b[43mbm25_bo1_pl2_monot5\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtopics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDone, run was created\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/pyterrier/transformer.py:223\u001b[0m, in \u001b[0;36mTransformer.__call__\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;124;03m    Sets up a default method for every transformer, which is aliased to transform() (for DataFrames)\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;124;03m    or transform_iter() (for iterable dictionaries) depending on the type of input. \u001b[39;00m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28minput\u001b[39m, pd\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[0;32m--> 223\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform_iter(\u001b[38;5;28minput\u001b[39m)\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/pyterrier/ops.py:335\u001b[0m, in \u001b[0;36mComposedPipeline.transform\u001b[0;34m(self, topics)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\u001b[38;5;28mself\u001b[39m, topics):\n\u001b[1;32m    334\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels:\n\u001b[0;32m--> 335\u001b[0m         topics \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtopics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    336\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m topics\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/pyterrier/ops.py:335\u001b[0m, in \u001b[0;36mComposedPipeline.transform\u001b[0;34m(self, topics)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\u001b[38;5;28mself\u001b[39m, topics):\n\u001b[1;32m    334\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels:\n\u001b[0;32m--> 335\u001b[0m         topics \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtopics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    336\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m topics\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/pyterrier_t5/__init__.py:60\u001b[0m, in \u001b[0;36mMonoT5ReRanker.transform\u001b[0;34m(self, run)\u001b[0m\n\u001b[1;32m     54\u001b[0m enc[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdecoder_input_ids\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfull(\n\u001b[1;32m     55\u001b[0m     (\u001b[38;5;28mlen\u001b[39m(queries[rng]), \u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdecoder_start_token_id,\n\u001b[1;32m     57\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong\n\u001b[1;32m     58\u001b[0m )\n\u001b[1;32m     59\u001b[0m enc \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m enc\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m---> 60\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     61\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39menc)\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m     62\u001b[0m result \u001b[38;5;241m=\u001b[39m result[:, \u001b[38;5;241m0\u001b[39m, (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mREL, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mNREL)]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/grad_mode.py:83\u001b[0m, in \u001b[0;36mno_grad.__exit__\u001b[0;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type: Any, exc_value: Any, traceback: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 83\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_grad_enabled\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprev\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/grad_mode.py:184\u001b[0m, in \u001b[0;36mset_grad_enabled.__init__\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, mode: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprev \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mis_grad_enabled()\n\u001b[0;32m--> 184\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_grad_enabled\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m=\u001b[39m mode\n",
      "\u001b[0;31mRuntimeError\u001b[0m: unknown parameter type"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mDer Kernel ist beim Ausführen von Code in der aktuellen Zelle oder einer vorherigen Zelle abgestürzt. Bitte überprüfen Sie den Code in der/den Zelle(n), um eine mögliche Fehlerursache zu identifizieren. Klicken Sie <a href='https://aka.ms/vscodeJupyterKernelCrash'>hier</a>, um weitere Informationen zu erhalten. Weitere Details finden Sie in Jupyter <a href='command:jupyter.viewOutput'>log</a>."
     ]
    }
   ],
   "source": [
    "print('Create run')\n",
    "run = bm25_bo1_pl2_monot5(topics)\n",
    "print('Done, run was created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Persist run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I use the environment variable \"TIRA_OUTPUT_DIR\" to determine where I should store the run file using \".\" as default.\n",
      "Done. run file is stored under \"./run.txt\".\n"
     ]
    }
   ],
   "source": [
    "persist_and_normalize_run(run, 't5-reranker')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
